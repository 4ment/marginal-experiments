---
title: "DS[1,2,3,3s,4,5]"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: pdf_document
toc: true
---

```{r setup, include=FALSE}
library(reshape2)
library(ggplot2)
library(knitr)
library(tidyr)
library(dplyr)
library(kableExtra)
library(ape)
knitr::opts_chunk$set(echo = TRUE, fig.path='Figs/')
```
\newpage

# The dubious methods

- SRF: posterior distribution cacluated with golden runs
- NMC: naive Monte Carlo with 10,000 samples
- ELBO: evidence lower bound (mean field Gaussian)
- HM: harmonic mean
- SHM: stabilized harmonic mean
- SS: stepping stone with 50 power posteriors
- GSS: generalized stepping stone with 50 power posterior. Reference distribution is independent gamma distributions.
- PS: path sampling with beta(0.3,1) and 50 power posteriors
- PS2: modified path sampling with beta(0.3,1) and 50 power posteriors
- ML: maximum likleihood
- MAP: maximum a posteriori using exp(10) as prior on each branch
- BS: bridge sampling
- NS: nested sampling
- CPO: conditional predictive ordinate
- LPPD: log pointwise predictive density
- GL: gamma Laplace
- LL: lognormal Laplace
- BL: beta' Laplace
- GLIS: gamma Laplace with importance sampling (10,000 samples)
- VBIS: Importance sampling using a variational distribution as the importance distribution 10,000 samples

```{r}
log.sum.exp<- function(x) {
  # Computes log(sum(exp(x))
  # Uses offset trick to avoid numeric overflow: http://jblevins.org/notes/log-sum-exp
  if ( max(abs(x)) > max(x) )
    offset <- min(x)
  else
    offset <- max(x)
  log(sum(exp(x - offset))) + offset
}

kl.log.pq <- function(P,Q, normalize=FALSE){
  if(normalize){
    Q = Q - log.sum.exp(Q)
    P = P - log.sum.exp(P)
  }
  sum(exp(P)*(P-Q))
}

h.log.pq <-function(P, Q, normalize=FALSE){
  if(normalize){
    Q = Q - log.sum.exp(Q)
    P = P - log.sum.exp(P)
  }
  sqrt(sum( (sqrt(exp(P)) - sqrt(exp(Q)))^2 ))/sqrt(2)
}

rmsd <- function(tree, P, Q, ptrees, pp){
  Q = Q - log.sum.exp(Q)
  P = P - log.sum.exp(P)
  
  Q = exp(Q[order(tree)])
  P = exp(P[order(tree)])
  
  msd = 0
  for(i in 1:length(pp)){
    msdi = 0
    for(j in 1:length(ptrees)){
      if(list(pp[[i]]) %in% ptrees[[j]]){
        msdi = msdi + Q[j] - P[j]
      }
    }
    msd = msd + msdi*msdi
  }
  sqrt(msd/length(pp))
}

read.ds <- function(dataset, force=F){
  #trees.file  = file.path('data', paste0(dataset, ".trees", sep=""))
  trees.file = paste0('/Users/mathieu/Desktop/marginal-experiments/data/JC_no_gamma_credible_set_ds', substr(dataset, nchar(dataset), nchar(dataset)))
  if(file.exists(paste0(dataset, '.Rdata')) & force!=T){
    load(paste0(dataset, '.Rdata'))
    return(res)
  }
  
  df = read.csv(file.path(dataset, paste(dataset, ".csv", sep="")), sep='\t')
  
  srf = read.csv(file.path(dataset, "data.csv"), sep='\t')
  srf = srf %>% mutate(SRF=log(SRF))
  df = mutate(df, rep=rep+1)

  dfff = df %>% filter(algorithm %in% c('mcmc', 'mmcmc', 'mmcmc-gss'))%>%
    spread(algorithm,time) %>% select(-marginal)
  df = filter(df, !(algorithm %in% c('mcmc', 'mmcmc', 'mmcmc-gss')))
  dfff = dfff %>% right_join(df, by=c('rep','tree'))
  #MMCMC
  for(m in c('SS', 'PS', 'PS2')){
    dfff[dfff$algorithm==m,]$time  = dfff[dfff$algorithm==m,]$time + 
      dfff[dfff$algorithm==m,]$mmcmc
  }
  #MCMC
  for(m in c('BS', 'CPO', 'LPPD', 'HM', 'SHM')){
    dfff[dfff$algorithm==m,]$time  = dfff[dfff$algorithm==m,]$time + 
      dfff[dfff$algorithm==m,]$mcmc
  }
  dfff[dfff$algorithm=='GSS',]$time  = dfff[dfff$algorithm=='GSS',]$time +
    dfff[dfff$algorithm=='GSS',]$`mmcmc-gss`
  
  df.time = dfff %>% group_by(algorithm) %>% 
    summarise(median=median(time),mean=mean(time), sd=sd(time), cv=sd(time)/mean(time)) %>%
    arrange(algorithm)
  
  # KL
  df.kl = dfff %>% full_join(srf, by='tree') %>% group_by(rep, algorithm) %>%
    summarize(KL=kl.log.pq(SRF, marginal, TRUE), H=h.log.pq(SRF, marginal, TRUE)) %>%
    arrange(algorithm) %>% mutate(dataset=dataset)
  
  #RMSD
  trees = read.tree(trees.file, keep.multi =TRUE)
  ptrees = list()
  for(i in seq_along(trees)){
    ptrees[[i]] = prop.part(trees[[i]])
  }
  pp=prop.part(trees)
  
  df.rmsd = dfff %>% full_join(srf, by='tree') %>% group_by(rep, algorithm) %>%
    summarize(RMSD=rmsd(tree, SRF, marginal, ptrees, pp)) %>%
    arrange(algorithm) %>% mutate(dataset=dataset)

  # plot
  myorder = df.kl %>% filter(rep==1) %>% arrange(KL) %>% ungroup() %>%
    select(algorithm) %>% unlist(use.names = FALSE) %>% as.vector
  
  df.plot = dfff %>% full_join(srf, by='tree') %>% 
    mutate(algorithm=factor(algorithm, levels=myorder))

  res = list(time=df.time, kl=df.kl, plot=df.plot, marginals=dfff,rmsd=df.rmsd)
  save(res, file=paste0(dataset, '.Rdata'))
  res
}

datasets = paste0('DS', 1:5)
```

```{r, include=FALSE}
out = NULL
for(i in 1:length(datasets)){
  out = c(out, knit_expand('DSx.Rmd'))
}
```

`r paste(knit(text = out), collapse = '\n')`

\newpage

# DS1 to DS5 together

```{r load-all}
df = read.ds(datasets[1])
all.kl = df[['kl']]
all.rmsd = df[['rmsd']]
all.time = df[['time']]
for(i in 2:length(datasets)){
  df = read.ds(datasets[i])
  all.kl = rbind(all.kl, df[['kl']])
  all.rmsd = rbind(all.rmsd, df[['rmsd']])
  all.time = rbind(all.time, df[['time']])
}
```

\newpage

## RMSD of 10 replicates
```{r method-rmsd-all}
all.rmsd$rep = factor(all.rmsd$rep, levels=unique(all.rmsd$rep))
all.rmsd.determ = filter(all.rmsd, algorithm %in% c('ML', 'MAP', 'GL', 'LL', 'BL') & rep==1)
all.rmsd = filter(all.rmsd, !(algorithm %in% c('ML', 'MAP', 'GL', 'LL', 'BL')))
all.rmsd = rbind(all.rmsd, all.rmsd.determ)
myorder = all.rmsd %>% group_by(algorithm) %>% summarize(medianRMSD=median(RMSD)) %>%
  arrange(medianRMSD) %>% select(algorithm) %>% unlist(use.names = FALSE) %>% as.vector

all.rmsd$Method = factor(all.rmsd$algorithm, levels=myorder)
ggplot(all.rmsd, aes(x=Method, y=RMSD,col=dataset)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1), legend.position = c(0.9, 0.2), 
        legend.title=element_blank(), legend.background=element_blank(),
        legend.key = element_rect(fill = NA, colour = NA, size = 0.25)) +
  geom_hline(yintercept = 0.01) +
  geom_hline(yintercept = 0.05, linetype="dashed") +
  geom_jitter(width = 0.3, size=1) +
  scale_y_log10()
```

\newpage

## KL of 10 replicates
```{r method-kl-all}
all.kl$rep = factor(all.kl$rep, levels=unique(all.kl$rep))
all.kl.determ = filter(all.kl, algorithm %in% c('ML', 'MAP', 'GL', 'LL', 'BL') & rep==1)
all.kl = filter(all.kl, !(algorithm %in% c('ML', 'MAP', 'GL', 'LL', 'BL')))
all.kl = rbind(all.kl, all.kl.determ)
myorder = all.kl %>% group_by(algorithm) %>% summarize(medianKL=median(KL)) %>%
  arrange(medianKL) %>% select(algorithm) %>% unlist(use.names = FALSE) %>% as.vector

all.kl$Method = factor(all.kl$algorithm, levels=myorder)
ggplot(all.kl, aes(x=Method, y=KL,col=dataset)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1), legend.position = c(0.9, 0.2), 
        legend.title=element_blank(), legend.background=element_blank(),
        legend.key = element_rect(fill = NA, colour = NA, size = 0.25)) +
  geom_jitter(width = 0.3, size=1) +
  scale_y_log10()
```

\newpage

## Method against running time (log scale)
```{r method-time, eval=FALSE, include=FALSE}
ds1 = read.ds(datasets[1])
all.time = as.data.frame(ds1[['time']]$median)
for(i in 2:length(datasets)){
  ds.time = read.ds(datasets[i])
  all.time = cbind(all.time, ds.time[['time']]$median)
}

myorder = rowMeans(all.time)
myorder = apply(all.time, 1, median)
all.time = log(all.time)
all.time = cbind(ds1[['time']]$algorithm, all.time)
colnames(all.time) = c("Method", datasets)
all.time2 = gather(all.time, Dataset, Time, -Method)
all.time2$Method = factor(all.time2$Method, levels=all.time$Method[order(myorder,decreasing=T)])
ggplot(all.time2, aes(x=Method)) + geom_point(aes(y=Time,col=Dataset)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
```
